<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression</h1>
            <p class="authors">
                Jia Wang<sup>1</sup>, Xinfeng Zhang<sup>1, *</sup>, Gai Zhang<sup>1</sup>, Jun Zhu<sup>1</sup>, Lv Tang<sup>1</sup>, Li Zhang<sup>2</sup>
            </p>
            <p class="affiliations">
                <sup>1</sup>University of Chinese Academy of Sciences, Beijing, China &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>Bytedance Inc., San Diego, USA
            </p>
            <p class="note"><sup>*</sup>Corresponding author</p>
            <div class="links">
                <a href="https://ieeexplore.ieee.org/document/11142881" target="_blank">Paper</a>
                <a href="https://arxiv.org/abs/2503.02733" target="_blank">arXiv</a>
                <a href="https://github.com/wj-inf/UAR-NVC" target="_blank">Code</a>
            </div>
        </header>

        <section class="video-player-section">
            <h2>Visual Comparison</h2>
            <p>Click the buttons below to switch between the baseline result and our result.</p>
            
            <!-- Video container -->
            <div class="video-player-container">
                <video id="videoPlayer" controls loop autoplay muted playsinline>
                    <!-- Source will be set by JavaScript -->
                </video>
            </div>
            
            <!-- Video switch buttons -->
            <div class="video-switch-buttons">
                <button class="video-switch-button active" id="btnVideo1" onclick="switchVideo(0)">HiNeRV (base) 0.371bpp 35.29dB</button>
                <button class="video-switch-button" id="btnVideo2" onclick="switchVideo(1)">HiNeRV (ours) 0.293bpp 40.21dB</button>
            </div>
        </section>

        <main>
        <section id="abstract">
            <h2>Abstract</h2>
            <p>Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference increases substantially, posing challenges in resource-constrained scenarios. Inspired by the success of traditional video compression frameworks, which process video frame by frame and can efficiently compress long videos, we adopt this modeling strategy for INRs to decrease memory consumption, while aiming to unify the frameworks from the perspective of timeline-based autoregressive modeling. In this work, we present a novel understanding of INR models from an autoregressive (AR) perspective and introduce a Unified AutoRegressive Framework for memory-efficient Neural Video Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural video compression under a unified autoregressive paradigm. It partitions videos into several clips and processes each clip using a different INR model instance, leveraging the advantages of both compression frameworks while allowing seamless adaptation to either in form. To further reduce temporal redundancy between clips, we treat the corresponding model parameters as proxies for these clips, and design two modules to optimize the initialization, training, and compression of these model parameters. In special, the Residual Quantization and Entropy Constraint (RQEC) module dynamically balances the reconstruction quality of the current clip and the newly introduced bitrate cost using the previously optimized parameters as conditioning. In addition, the Interpolation-based Initialization (II) module flexibly adjusts the degree of reference used during the initialization of neighboring video clips, based on their correlation. UAR-NVC supports adjustable latencies by varying the clip length. Extensive experimental results demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to resource-constrained environments and significantly improve performance compared to different baseline models.</p>
        </section>
        <div class="contributions-and-fig1">
            <div class="figure-container">
                <div class="image-container">
                    <img src="images/figure1.png" alt="Figure 1: Different AutoRegressive (AR) methods">
                    <p class="caption">Figure 1: Different AutoRegressive (AR) methods: (a) Pixel-level AR. (b) Frame-level AR. (c) Implicit domain (INR-based) AR. (d) Our UAR-NVC, which integrates INR-based AR and timeline-based AR by using INR models to represent each video clip and applying AR between INR models to capture inter-clip relationships.</p>
                </div>
            </div>
            <div class="contributions-container">
                <section id="contributions">
                    <p>The contributions of our work are summarized as follows:</p>
                    <ul>
                        <li><strong>Unified Framework:</strong> We propose UAR-NVC, a novel framework that unifies timeline-based and INR-based NVC frameworks. Most existing INR model can be seamlessly integrated into our framework, demonstrating its flexibility and generality.</li>
                        <li><strong>Enhancements in Autoregressive Modeling:</strong> We design two modules to enhance autoregressive modeling for videos under the proposed UAR-NVC framework. The RQEC module dynamically balances the newly introduced information and the distortion of the current clip, based on the INR model parameters of the previous clip. The II module adaptively controls the reference strength for INR model initialization, allowing it to handle diverse video data effectively.</li>
                        <li><strong>Comprehensive Experiments:</strong> We conduct extensive experiments to verify the efficiency of the proposed UAR-NVC framework. The results demonstrate its superior performance in rate-distortion optimization and highlight its advantages for practical applications.</li>
                    </ul>
                </section>
            </div>
        </div>

        <section id="method">
            <h2>Method</h2>
            <div class="image-container">
                <img src="images/UAR-NVC_framework.png" alt="Figure 2: The proposed UAR-NVC framework">
                <p class="caption">Figure 2: The proposed UAR-NVC, a practical INR framework for video compression. The video frames are grouped into several GOPs (video clips), where we train one model for one GOP. To balance correlation capture and random access, GOPs are grouped into GOMs. And time dependency exists only between GOPs within a GOM. The right part shows the training and compression pipeline of one GOM in UAR-NVC.</p>
            </div>
            <p>Our UAR-NVC framework unifies timeline-based and INR-based neural video compression from an autoregressive perspective. The video is partitioned into clips (GOPs), each modeled by a separate INR model instance. To leverage correlation between clips and reduce redundancy, we introduce two key modules:</p>
            <ul>
                <li><strong>Residual Quantization and Entropy Constraint (RQEC):</strong> This module uses the parameters of the previously optimized model as conditioning to dynamically balance the reconstruction quality of the current clip and the bitrate cost of the newly introduced information. It stores the difference (residual) between the initialized and optimized parameters for efficient compression.</li>
                <li><strong>Interpolation-based Initialization (II):</strong> This module adaptively controls the reference strength for INR model initialization based on the correlation between neighboring clips. It interpolates between the previous model's initial and optimized parameters to provide a better starting point for training.</li>
            </ul>
            <p>The framework also incorporates the concept of Group of Models (GOM), analogous to Group of Pictures (GOP) in traditional codecs, to support random access.</p>
            <div class="image-row">
                <div class="image-container fig3-container">
                    <img src="images/GOM.png" alt="Figure 3: GOM level video partition">
                    <p class="caption">Figure 3: (a): GOP level video partition with I-frames and P-frames. (b): GOM level video partition with I-models(#1/4) and P-models(#2/3/5/6).</p>
                </div>
                <div class="image-container fig4-container">
                    <img src="images/RQAT_version2.png" alt="Figure 4: Residual quantization-based entropy estimation">
                    <p class="caption">Figure 4: Residual quantization-based entropy estimation on P-model.</p>
                </div>
            </div>
        </section>

        <section id="results">
            <h2>Results</h2>
            <p>Experimental results on the UVG and VVC Class B datasets demonstrate the effectiveness of UAR-NVC. When integrated with base INR models like NeRV, HNeRV, and HiNeRV, our framework achieves significant improvements in rate-distortion (RD) performance compared to their baseline versions. Key findings include:</p>
            <ul>
                <li>Substantial BD-rate savings (up to 64.97% for HiNeRV on UVG) across different base models and GOP sizes.</li>
                <li>Superior performance compared to traditional codecs like H.265/x265 (veryslow), especially at larger GOP sizes.</li>
                <li>Effective balance between memory efficiency (due to clip-based processing) and compression performance, enabled by the RQEC and II modules.</li>
                <li>Flexibility to adapt to different resource constraints by varying the clip length (p).</li>
            </ul>
            <div class="image-container">
                <img src="images/Table1.png" alt="Table 1: BD-rate (\%) for PSNR/MS-SSIM on different datasets and p (GOP).">
                <p class="caption">Table 1: BD-rate (%) for PSNR/MS-SSIM on different datasets and p (GOP).</p>
            </div>
            <div class="image-container">
                <img src="images/Table2.png" alt="Table 2: Rate Distortion performance on different sequence of UVG.">
                <p class="caption">Table 2: Rate Distortion performance on different sequence of UVG.</p>
            </div>
            <div class="image-container">
                <img src="images/selected_images2.png" alt="Figure 8: Subjective quality comparison">
                <p class="caption">Figure 8: Subjective quality comparison between HiNeRV (base) and HiNeRV (ours).</p>
            </div>
            
            <div class="image-container">
                <img src="images/VVC_main_result.png" alt="Figure 9: Rate Distortion performance on VVC ClassB dataset">
                <p class="caption">Figure 9: Rate Distortion performance on VVC ClassB dataset with different p (GOP).</p>
            </div>
        </section>
        </main>

        <footer>
            <p>&copy; 2025 UAR-NVC Project</p>
            <pre id="citation">
@article{wang2025uar,
  title={UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression},
  author={Wang, Jia and Zhang, Xinfeng and Zhang, Gai and Zhu, Jun and Tang, Lv and Zhang, Li},
  journal={arXiv preprint arXiv:2503.02733},
  year={2025}
}
</pre>
        </footer>
    </div>

    <script>
        const videos = [
            "videos2/bunny-HiNeRV-base-ms0.3MB-bpp0.371-PSNR35.294dB.mp4",
            "videos2/bunny-HiNeRV-ours-ms0.9MB-bpp0.293-PSNR40.315dB.mp4",
        ];

        let currentVideoIndex = 0;
        const videoPlayer = document.getElementById('videoPlayer');
        const buttons = [
            document.getElementById('btnVideo1'),
            document.getElementById('btnVideo2')
        ];

        function switchVideo(index) {
            if (index === currentVideoIndex) return; // Do nothing if clicking the same button

            currentVideoIndex = index;
            
            // Update button states
            buttons.forEach((button, i) => {
                if (i === currentVideoIndex) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });

            // Change video source
            videoPlayer.src = videos[currentVideoIndex];
            videoPlayer.load();
            videoPlayer.play().catch(error => {
                console.error("Autoplay was prevented:", error);
                // Autoplay was prevented. Show a "Play" button or similar.
            });
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', () => {
            videoPlayer.src = videos[0];
        });
    </script>
</body>
</html>